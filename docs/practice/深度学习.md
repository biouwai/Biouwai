# 深度学习

## 矩阵基础

1. 线性代数
   - 代数：代替数字，研究运算
   - 线性代数：代替向量，研究线性映射
2. 向量
   - 向量 × 同一个数有意义
   - 两向量各位置想加有意义
3. 线性映射
   - 一个集合 --> 另一个集合
   - 映射的像和原象都是向量
   - 输入 × k ==> 输出 × k
4. 鸡兔同笼问题

   $\begin{bmatrix}
   x \\
   y
   \end{bmatrix}=\begin{bmatrix}
   1x+1y \\
   2x+4y
   \end{bmatrix}$

- 鸡向量就是 1 头 2 脚，兔向量就是 1 头 4 脚，分别乘以对应数量，就可得到映射的输出总头数和总脚数
- 故$\begin{bmatrix}
   1,1 \\
   2,4
   \end{bmatrix}$就是将鸡兔数映射为头脚数的线性映射
- 且也满足两组组合相加有意义，数乘有意义

5. 矩阵 --> 线性映射的代名词
   - 矩阵每一列，就代表着线性映射的每一个向量，比如$\begin{bmatrix}
   1 \\
   2
   \end{bmatrix}$就代表 1 头 2 脚

## 神经网络

### 一、目标

得到一个可以由输入 x 得出输出 y 的“模型”。xy 可以是任意场景下的输入输出，如：

    > 1 号 --> 1 号西安车流量
    >
    > 小明近三天浏览记录 --> 要给小明推荐的视频

### 二、工作原理

一个模型可以看作非线性函数，一个非线性函数又是由多个非线性子函数组合而成，子函数也可以再由子函数组合而成。神经网络有足够的神经元，通过调整各个子函数的权重 ω 和偏置 b，就能大概模拟出一个我们想要的模型。

下图每一条线就为一个 ω x + b , 每个圈就表示一个神经元。神经元负责将线性函数转为非线性（激活）

![](/神经网络.png)

我们该如何训练一个可以模拟正确输出的模型呢

1. 初始化，随机各 ω 和 b，经过一轮输出得到 y^

2. 计算 loss， 通过输出 y^ 和 真实 y 计算出 loss

3. 减小 loss，loss 越小，说明输出 y^越接近真实值。

### 三、关键点

模型训练的关键点就在与减小 loss，就有各种算法去减小 loss

如：

对 loss 求偏导，如果 loss 对某个参数的偏导为正，则该参数越大，loss 越大。所以就要减小该参数。

以此不断改变权重 ω 和偏置 b，使 loss 减小到某一区间，就完成了模型的训练

## GCN

### 一、可以做什么

对特征进行提取

1. 结点分类
   ![](/GCN1.jpg)
   例：上图是一个社交网络，节点表示成员的特征，边表示有社交关系，颜色代表了他们所属的小团体。不依靠节点的特征，只通过图，在随机初始化权重之后，经过两次前向传播，可以大致将成员分对。
2. 关联预测：预测两个结点之间是否存在连接。

### 二、主要思想

对于每个结点，我们都要考虑其所有邻居以及其自身所包含的特征信息。使聚合后的结点特征更准确

### 三、聚合公式

![](/GCN.png)

**$一个结点的特征=\sum\frac{相邻结点的特征×权重}{\sqrt{节点边个数×邻居节点边个数}}。$**

以此对整个图结构进行聚合操作。

我们就能得到整个图结构每个结点的深层特征，由此我们就能将每个简单进行分类，预测两个结点间的联系等

## SVD

### 一、 定义

1. SVD：

一个矩阵就是一个线性变换，一般包含了伸缩变化和旋转变化。

为了使更好的对矩阵进行分析，我们可以将矩阵分解成单独的伸缩变换和旋转变换再进行研究，SVD 就是以此分解矩阵的。

如下图：正方块在矩阵的的线性映射下变为了右上角图。就可以分解为以下三步:

（1）在正交矩阵 QT 作用下选择了 90 度

（2）在对角矩阵 S 作用下伸缩了 n 倍

（3）再在正交矩阵 Q 作用下逆时针旋转了 90 度

![](/SVD几何.png)

由此，就将一个一般的矩阵，分解成了一个正交矩阵 × 对角矩阵 × 正交矩阵

2. 奇异值：

   奇异值可以理解为主特征值，相当于每个维度伸缩的大小。

   奇异值越大，伸缩得越大，说明在此维度的变化越大，当然也就越重要。

   所以奇异值在对角阵主对角线从大到小排列，有时可以忽略后面的奇异值，也能比较完整的还原原矩阵。

### 二、 思想

1. 矩阵视角：

当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前 N 个特征向量，那么就对应了这个矩阵最主要的 N 个变化方向。我们利用这前 N 个变化方向，就提取了数据的本质特征，就可以近似这个矩阵（变换）。同时，去除影响小的变化方向也有利于降噪、图片压缩等方向的应用。

2. 思想：

找到本质特征，实现问题简化

### 三、 链接预测问题中的作用

链接预测通常涉及处理高维的网络数据，例如推荐系统中的用户电影-矩阵（一个用户可能看过很多电影），生物网络中的蛋白质相互作用矩阵等。这些矩阵往往具有很高的维度。

而通过 svd 分解，将推荐系统中的用户-电影矩阵分解为用户-喜好矩阵与电影-类型矩阵，就将很稀疏的用户-电影矩阵进行了降维，提取出了用户的主要喜好（主要特征），从而使我们能简化运算的同时又保证了预测的准确性。

![](/svd推荐系统.png)

## 对比学习

### 一、思想

人类小孩很容易分辨猫和狗，是因为猫和狗有个别特征很不一样，如狗的脸比较突出，猫的毛发比较顺滑等，通过比较这些高阶特征我们就能很快辨别猫狗，而不用从头到尾的分析。

对比学习就是通过对比数据对的“相似”或“不同”以获取数据的高阶信息。通过训练，让正向数据对经过模型后更靠近，负向数据对离更远，以此让模型具有对应数据的判别能力。

### 二、工作原理

1.  数据扩增(增强)：例如同一张猫的照片可以通过调色裁剪等得到另外两张照片

2.  编码：将每张图片丢进卷积神经网络中，都会得到一个向量表示。

3.  损失函数最小化：

    希望正向数据对的向量表示尽可能的相似 --> 两向量的夹角尽量大（余弦值靠近 1）

    负向数据对的向量表示尽可能的不同 --> 两向量的夹角尽量小（余弦值靠近-1）

    以此去调整损失函数。

### 优势

自监督学习，可以用无标注的数据集进行学习
