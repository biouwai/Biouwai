# 深度学习

## 矩阵基础

1. 线性代数
   - 代数：代替数字，研究运算
   - 线性代数：代替向量，研究线性映射
2. 向量
   - 向量 × 同一个数有意义
   - 两向量各位置想加有意义
3. 线性映射
   - 一个集合 --> 另一个集合
   - 映射的像和原象都是向量
   - 输入 × k ==> 输出 × k
4. 鸡兔同笼问题

   $\begin{bmatrix}
   x \\
   y
   \end{bmatrix}=\begin{bmatrix}
   1x+1y \\
   2x+4y
   \end{bmatrix}$

- 鸡向量就是 1 头 2 脚，兔向量就是 1 头 4 脚，分别乘以对应数量，就可得到映射的输出总头数和总脚数
- 故$\begin{bmatrix}
   1,1 \\
   2,4
   \end{bmatrix}$就是将鸡兔数映射为头脚数的线性映射
- 且也满足两组组合相加有意义，数乘有意义

5. 矩阵 --> 线性映射的代名词
   - 矩阵每一列，就代表着线性映射的每一个向量，比如$\begin{bmatrix}
   1 \\
   2
   \end{bmatrix}$就代表 1 头 2 脚

## 神经网络

### 一、目标

得到一个可以由输入 x 得出输出 y 的“模型”。xy 可以是任意场景下的输入输出，如：

    > 1 号 --> 1 号西安车流量
    >
    > 小明近三天浏览记录 --> 要给小明推荐的视频

### 二、工作原理

一个模型可以看作非线性函数，一个非线性函数又是由多个非线性子函数组合而成，子函数也可以再由子函数组合而成。神经网络有足够的神经元，通过调整各个子函数的权重 ω 和偏置 b，就能大概模拟出一个我们想要的模型。

下图每一条线就为一个 ω x + b , 每个圈就表示一个神经元。神经元负责将线性函数转为非线性（激活）

![](/神经网络.png)

我们该如何训练一个可以模拟正确输出的模型呢

1. 初始化，随机各 ω 和 b，经过一轮输出得到 y^

2. 计算 loss， 通过输出 y^ 和 真实 y 计算出 loss

3. 减小 loss，loss 越小，说明输出 y^越接近真实值。

### 三、关键点

模型训练的关键点就在与减小 loss，就有各种算法去减小 loss

如：

对 loss 求偏导，如果 loss 对某个参数的偏导为正，则该参数越大，loss 越大。所以就要减小该参数。

以此不断改变权重 ω 和偏置 b，使 loss 减小到某一区间，就完成了模型的训练

## GCN

### 一、可以做什么

对特征进行提取

1. 结点分类
   ![](/GCN1.jpg)
   例：上图是一个社交网络，节点表示成员的特征，边表示有社交关系，颜色代表了他们所属的小团体。不依靠节点的特征，只通过图，在随机初始化权重之后，经过两次前向传播，可以大致将成员分对。
2. 关联预测：预测两个结点之间是否存在连接。

### 二、主要思想

对于每个结点，我们都要考虑其所有邻居以及其自身所包含的特征信息。使聚合后的结点特征更准确

### 三、聚合公式

![](/GCN.png)

**$一个结点的特征=\sum\frac{相邻结点的特征×权重}{\sqrt{节点边个数×邻居节点边个数}}。$**

以此对整个图结构进行聚合操作。

我们就能得到整个图结构每个结点的深层特征，由此我们就能将每个简单进行分类，预测两个结点间的联系等

## SVD

### 一、 定义

1. SVD：

一个矩阵就是一个线性变换，一般包含了伸缩变化和旋转变化。

为了使更好的对矩阵进行分析，我们可以将矩阵分解成单独的伸缩变换和旋转变换再进行研究，SVD 就是以此分解矩阵的。

如下图：正方块在矩阵的的线性映射下变为了右上角图。就可以分解为以下三步:

（1）在正交矩阵 QT 作用下选择了 90 度

（2）在对角矩阵 S 作用下伸缩了 n 倍

（3）再在正交矩阵 Q 作用下逆时针旋转了 90 度

![](/SVD几何.png)

由此，就将一个一般的矩阵，分解成了一个正交矩阵 × 对角矩阵 × 正交矩阵

2. 奇异值：

   奇异值可以理解为主特征值，相当于每个维度伸缩的大小。

   奇异值越大，伸缩得越大，说明在此维度的变化越大，当然也就越重要。

   所以奇异值在对角阵主对角线从大到小排列，有时可以忽略后面的奇异值，也能比较完整的还原原矩阵。

### 二、 思想

1. 矩阵视角：

当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前 N 个特征向量，那么就对应了这个矩阵最主要的 N 个变化方向。我们利用这前 N 个变化方向，就提取了数据的本质特征，就可以近似这个矩阵（变换）。同时，去除影响小的变化方向也有利于降噪、图片压缩等方向的应用。

2. 思想：

找到本质特征，实现问题简化

### 三、 链接预测问题中的作用

链接预测通常涉及处理高维的网络数据，例如推荐系统中的用户电影-矩阵（一个用户可能看过很多电影），生物网络中的蛋白质相互作用矩阵等。这些矩阵往往具有很高的维度。

而通过 svd 分解，将推荐系统中的用户-电影矩阵分解为用户-喜好矩阵与电影-类型矩阵，就将很稀疏的用户-电影矩阵进行了降维，提取出了用户的主要喜好（主要特征），从而使我们能简化运算的同时又保证了预测的准确性。

![](/svd推荐系统.png)

## 对比学习

### 一、思想

人类小孩很容易分辨猫和狗，是因为猫和狗有个别特征很不一样，如狗的脸比较突出，猫的毛发比较顺滑等，通过比较这些高阶特征我们就能很快辨别猫狗，而不用从头到尾的分析。

对比学习就是通过对比数据对的“相似”或“不同”以获取数据的高阶信息。通过训练，让正向数据对经过模型后更靠近，负向数据对离更远，以此让模型具有对应数据的判别能力。

### 二、工作原理

1.  数据扩增(增强)：例如同一张猫的照片可以通过调色裁剪等得到另外两张照片

2.  编码：将每张图片丢进卷积神经网络中，都会得到一个向量表示。

3.  损失函数最小化：

    希望正向数据对的向量表示尽可能的相似 --> 两向量的夹角尽量大（余弦值靠近 1）

    负向数据对的向量表示尽可能的不同 --> 两向量的夹角尽量小（余弦值靠近-1）

    以此去调整损失函数。

### 优势

自监督学习，可以用无标注的数据集进行学习

## lightGCL

### 研究背景

1. 图神经网络 GNN 在推荐系统表现良好，因为其可以很好的挖掘高阶连接信息以进行协同过滤。但很大部分基于 GNN 的模型都采用了监督学习范式，需要大量带准确标签的数据进行训练

2. 实际应用场景中，数据稀疏，导致监督学习无充分信息利用，就可以对比学习可以来进行数据增强，得到节点和图的重要特征。

3. 虽然对比学习在提升图推荐系统的效果上作用十分明显，它的效果很大程度上依赖于数据增强的方法。绝大部分现有的图对比学习方法使用基于随机过程或基于经验的数据增强方法。
   会有其缺点：

   a. 基于随机过程的图数据增强可能会损失图中重要的结构信息，从而误导模型；

   b. 基于经验的对比学习方法建立在对数据分布较强的假设上，而这会限制模型的普适性，并容易受数据中的噪声影响

4. 基于以上问题，论文提出 lightGCL，图数据增强是由奇异值分解及重构（Singular Value Decomposition and Reconstruction）来指导的。

   奇异值分解重构所得的新图为全连接图，不但能挖掘局部的用户-物品交互信息，而且可以提取全局的协同过滤信号；其次，以奇异值分解重构作为数据增强的方法相比于随机过程或基于经验的方法含有更多有效信息；最后，由于该数据增强方法有效保存了原图的信息，

   以将奇异值分解重构图产生的表征向量于原图产生的表征向量直接进行对比学习。将数据增强图的数量由两个减为一个，大大提升了训练效率。

### 模型介绍

![](/lightGCL.png)

1. 局部图结构信息提取

   与通常的协同过滤范式相类似，每个用户和物品都具有一个可学习的隐式表征向量，以表示结点的特征。LightGCL 采取了常用的双层图卷积网络 GCN，
   对用户与物品间局部的领域关系进行学习。在每一层图卷积网络中，每个结点的表征向量都会依着图的边传播到相邻结点。
   为了防止过拟合，LightGCL 采用了剩余连接的方法，使每个结点在信息传播和整合中不至于丧失自身的有效信息。

2. 基于奇异值分解的高效全局信息挖掘
   原始矩阵奇异值分解后，截取最大的 q 个奇异值，抛弃剩余的较小奇异值，并重构邻接矩阵。这样可以保留关键信息，去除扰动。

   这个重构的邻接矩阵实际上是原零阶矩阵的低阶近似，不仅包含了原邻接矩阵中的重要组分信息，而且由于其为全连接图，
   考虑了每一对用户和物品之间的潜在关联，更能挖掘图中的全局信息。
   鉴于以上的优点，LightGCL 采用了这个重构的邻接矩阵作为对比学习中的增强图。
   用近似奇异值算法进行奇异值分解避免计算很长时间。

3. 简化高效的对比学习
   传统的图对比学习方法（如 SGL，SimGCL 等）需要创建两个增强图，而在原图上产生的表征向量并不参与对比学习。这些模型之所以采用这种低效的结构，
   可能是因为他们**采用的基于随机过程的图增强方法可能对主任务的学习起到误导效果**。

   LightGCL 框架中，增强图实际上包含奇异值分解带来的有效信息，可以加强主任务的学习。
   因此，我们得以将奇异值分解重构图产生的表征向量于原图产生的表征向量直接进行对比学习。
   这样一来，模型只需计算一个增强图，大大简化了对比学习的范式。
